{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d5cfd8",
   "metadata": {},
   "source": [
    "<!-- LTeX: language=fr -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e60f73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Cours 6‚ÄØ: *Transformers*, principes et usage\n",
    "\n",
    "**Lo√Øc Grobol** [&lt;lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)\n",
    "\n",
    "2022-01-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741f8ee",
   "metadata": {},
   "source": [
    "## Prologue‚ÄØ: encodeurs-d√©codeurs et traduction automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb229cb8",
   "metadata": {},
   "source": [
    "R√©f√©rences‚ÄØ: Sutskever et al ([2014](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)) et Cho et al ([2014](http://aclweb.org/anthology/D14-1179))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc96fa",
   "metadata": {},
   "source": [
    "## M√©canisme d'attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8deda",
   "metadata": {},
   "source": [
    "R√©f√©rence‚ÄØ: Badhanau et al ([2015](http://arxiv.org/abs/1409.0473))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8199f",
   "metadata": {},
   "source": [
    "## *Attention is all you need*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5beda94",
   "metadata": {},
   "source": [
    "R√©f√©rence‚ÄØ: Vaswani et al. ([2017](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9cded",
   "metadata": {},
   "source": [
    "## Pr√©entra√Ænement et *fine-tuning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b3d9f",
   "metadata": {},
   "source": [
    "R√©f√©rences‚ÄØ: Peters et al. ([2018](http://aclweb.org/anthology/N18-1202)), Radford et al. ([2018](https://openai.com/blog/language-unsupervised)) Howard et Ruder ([2018](https://www.aclweb.org/anthology/P18-1031)), Devlin et al. ([2019](https://www.aclweb.org/anthology/N19-1423))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80df172",
   "metadata": {},
   "source": [
    "## ü§ó `transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54247a58",
   "metadata": {},
   "source": [
    "Cette partie est adapt√©e du [cours ü§ó](https://huggingface.co/course) que je vous encourage tr√®s fortement √† lire en d√©tails‚ÄØ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb60098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64457d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281c44a",
   "metadata": {},
   "source": [
    "### Charger et utiliser des mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d2c2c",
   "metadata": {},
   "source": [
    "On va utiliser DistilBERT ([Sahn et al, 2019](https://arxiv.org/abs/1910.01108)), une version de BERT compress√©e par distillation. Elle a l'avantage d'√™tre consid√©rablement plus l√©g√®re (donc gentille avec les notebooks), tout en √©tant presque aussi performante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed841e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = transformers.AutoModel.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99354ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Morgan reconnait l'existence du kiwi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    transformer_output = bert_model(**tokenizer(\"Morgan reconnait l'existence du kiwi.\", return_tensors=\"pt\"))\n",
    "transformer_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a87b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = transformer_output.last_hidden_state\n",
    "display(final_embeddings)\n",
    "display(final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    transformer_output = bert_model(\n",
    "        **tokenizer(\"Morgan reconnait l'existence du kiwi.\",return_tensors=\"pt\"),\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "transformer_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9e9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = transformer_output.hidden_states\n",
    "display(all_embeddings)\n",
    "display([e.shape for e in all_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour ne pas manger toute la RAM\n",
    "del bert_model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca115d7d",
   "metadata": {},
   "source": [
    "### Utiliser des *pipelines*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a175562",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = transformers.pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"There are not many sentiment analysis models on ü§ó hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = transformers.pipeline(\"fill-mask\", model=\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a83149",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(f\"En France, c'est l'Universit√© de {lm.tokenizer.mask_token} la meilleure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce6af8",
   "metadata": {},
   "source": [
    "Pour les autres‚ÄØ: voire [la liste des pipelines](https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/pipelines#transformers.pipeline.task) dans la doc.\n",
    "\n",
    "Attention‚ÄØ: tous les mod√®les n'ont pas √©t√©s affin√©s pour toutes les pipelines, vous pouvez chercher sur [le hub de ](https://huggingface.co/models) les mod√®les entra√Æn√©s pour une t√¢che et une langue donn√©e. Par exemple les mod√®les de [NER](https://huggingface.co/models?language=fr&sort=downloads&search=ner)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b4e26",
   "metadata": {},
   "source": [
    "Et s'il n'y en a pas‚ÄØ? On peut en entra√Æner un‚ÄØ!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978932f",
   "metadata": {},
   "source": [
    "## Affiner un mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f08cc",
   "metadata": {},
   "source": [
    "### Donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467c813",
   "metadata": {},
   "source": [
    "On va utiliser une des t√¢ches du multi-benchmark [GLUE](https://gluebenchmark.com), dans sa version [ü§ó datasets](https://huggingface.co/datasets/glue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42d9bf",
   "metadata": {},
   "source": [
    "Il y a beaucoupd de sous-t√¢ches dans GLUE (c'est le principe), on va commencer par regarder celle qui nous est peut-√™tre la plus famili√®re‚ÄØ: la d√©tection de polarit√© (*sentiment analysis*), avec le corpus [*Stanford Sentiment Treebank*](https://nlp.stanford.edu/sentiment/index.html) version 2, `sst2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8619fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273db5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset[16:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a664e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset[\"sentence\"][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e821ee",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = tokenizer(raw_datasets[\"train\"][\"sentence\"][9])\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_sent[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(raw_datasets[\"train\"][\"sentence\"])[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da87b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df290681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eb149",
   "metadata": {},
   "source": [
    "### Affinage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\", num_labels=2\n",
    ")\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = transformers.pipeline(\"sentiment-analysis\", model=classifier, tokenizer=tokenizer)\n",
    "sentiment_pipeline(\"This movie is not so bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=8,\n",
    "    max_steps=64,\n",
    "    output_dir=\"local/distilbert-base-multilingual-cased+sst2\",\n",
    "    per_device_train_batch_size=4,\n",
    "    report_to=\"none\",\n",
    "    warmup_ratio=1/16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab334c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    classifier,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956a1e2",
   "metadata": {},
   "source": [
    "### Utiliser le mod√®le entra√Æn√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e1ab6",
   "metadata": {},
   "source": [
    "Les param√®tres du mod√®le ont √©t√© directement modifi√©s et on peut l'utiliser tout de suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2eacff",
   "metadata": {},
   "source": [
    "Bien penser √† mettre le mod√®le en mode √©valuation, sinon les r√©sultats seront partiellement al√©atoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d37ce",
   "metadata": {},
   "source": [
    "On peut s'en servir pour faire des pr√©dictions directement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    classifier_output = classifier(**tokenizer(\"This movie is not so bad\", return_tensors=\"pt\"))\n",
    "    display(classifier_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c93c65",
   "metadata": {},
   "source": [
    "On peut aussi l'utiliser dans la pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0864aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline(\"This movie is not so bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17bd32",
   "metadata": {},
   "source": [
    "Le score pour la pipeline et le logit correspondant en appliquant directement le mod√®le sont diff√©rent, c'est parce que la pipeline applique un softmax‚ÄØ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_output.logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47654b",
   "metadata": {},
   "source": [
    "Si on veut utiliser le mod√®le affin√© ailleurs, il faut le sauvegarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_pretrained(\"local/distilbert-base-multilingual-cased+sst2/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd7996",
   "metadata": {},
   "source": [
    "On peut alors le charger avec le `.from_pretrained(\"local/distilbert-base-multilingual-cased+sst2/model\")` qui va bien, par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = transformers.AutoModelForSequenceClassification.from_pretrained(\"local/distilbert-base-multilingual-cased+sst2/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71613c8",
   "metadata": {},
   "source": [
    "C'est une bonne pratique de sauvegarder le tokenizer avec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"local/distilbert-base-multilingual-cased+sst2/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a129e4e",
   "metadata": {},
   "source": [
    "Et pour rendre votre mod√®le facilement r√©utilisable, vous pouvez le mettre sur [le hub](https://huggingface.co/docs/hub). Il y a aussi des int√©grations pour le faire directement dans le `Trainer`, voir [la doc](https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.Trainer.push_to_hub)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364fcf5",
   "metadata": {},
   "source": [
    "### √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(type(predictions.predictions), predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels == predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_labels == predictions.label_ids).sum()/predicted_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = datasets.load_metric(\"glue\", \"sst2\")\n",
    "metric.compute(predictions=predicted_labels, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164432e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = datasets.load_metric(\"glue\", \"sst2\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb648b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On r√©g√©n√®re le classifieur pour reprendre l'entra√Ænement de z√©ro\n",
    "\n",
    "classifier = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\", num_labels=2\n",
    ")\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=8,\n",
    "    max_steps=64,\n",
    "    output_dir=\"local/distilbert-base-mutlitlingual-cased+sst2\",\n",
    "    per_device_train_batch_size=4,\n",
    "    report_to=\"tensorboard\",\n",
    "    warmup_ratio=1/16,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    args=training_args,    \n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    model=classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_datasets[\"train\"],   \n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79807c43",
   "metadata": {},
   "source": [
    "Vous pouvez voir l'√©volution de l'entra√Ænement dans [`tensorboard`](https://www.tensorflow.org/tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0980e",
   "metadata": {},
   "source": [
    "```console\n",
    "tensorboard serve --logdir slides/06-transformers/local/distilbert-base-multilingual-cased+sst2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae18dd",
   "metadata": {},
   "source": [
    "### √Ä vous de jouer\n",
    "\n",
    "Affinez un mod√®le (pour un nombre raisonnable de pas) en fran√ßais ou multilingue sur la t√¢che de d√©tection de polarit√© du benchmark [FLUE](https://huggingface.co/datasets/flue#text-classification-cls) ([Le et al, 2020](https://hal.archives-ouvertes.fr/hal-02890258))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "del sentiment_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca2aae",
   "metadata": {},
   "source": [
    "## Pr√©entra√Æner un mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325407dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = transformers.AutoConfig.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e171efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForMaskedLM.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = transformers.pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08141fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(f\"En France, c'est l'Universit√© de {lm.tokenizer.mask_token} la meilleure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p local\n",
    "!wget \"https://sharedocs.huma-num.fr/wl/?id=LLYeokePZiJytROQ41iI3fkss6lMmGwd&fmode=download\" -O local/ESLO_raw.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head local/ESLO_raw.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = datasets.load_dataset(\"text\", data_files=[\"local/ESLO_raw.txt\"])\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff07e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], truncation=True)\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_function, batched=True\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30741b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [tokenized_dataset[\"train\"][i] for i in range(8)]\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eed848",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=2,\n",
    "    max_steps=16,\n",
    "    output_dir=f\"local/distilbert-ESLO\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    report_to=\"tensorboard\",\n",
    "    warmup_ratio=1/8,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37e048",
   "metadata": {},
   "source": [
    "## Entra√Æner des tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_tokenizer = tokenizer.train_new_from_iterator(raw_dataset[\"train\"], 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Je reconnais l'existence du kiwi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_tokenizer.tokenize(\"Je reconnais l'existence du kiwi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c20ab",
   "metadata": {},
   "source": [
    "## Autres outils\n",
    "\n",
    "- Pytorch Lightning et [Lightning Transformers](https://lightning-transformers.readthedocs.io/en/latest/)\n",
    "- [`simpletransformers`](https://simpletransformers.ai)\n",
    "- [Zelda Rose](https://github.com/LoicGrobol/zeldarose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fb8bc",
   "metadata": {},
   "source": [
    "## Biais et limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fe86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = transformers.pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "                                 \n",
    "result = unmasker(\"This person works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13692fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = transformers.pipeline(\"fill-mask\", model=\"distilbert-base-multilingual-cased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "                                 \n",
    "result = unmasker(\"This person works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
